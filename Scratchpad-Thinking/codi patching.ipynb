{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7252da17-4e85-4ac0-85b1-7114f1f00ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Class Definitions\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "from accelerate.utils import set_seed\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(default=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    lora_r: int = field(default=128, metadata={\"help\": \"lora rank\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"lora dropout\"})\n",
    "    full_precision: bool = field(default=True, metadata={\"help\": \"whether use int4 for the base model\"})\n",
    "    lora_init: bool = field(default=False, metadata={\"help\": \"True: Use zero and gaussian initialization\"})\n",
    "    token: Optional[str] = field(default=None, metadata={\"help\": \"HF token to access private models\"})\n",
    "    adapter_name_or_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to the LoRA adapter\"})\n",
    "    lora_alpha: int = field(default=16, metadata={\"help\": \"LoRA alpha\"})\n",
    "    ckpt_dir: Optional[str] = field(default=None, metadata={\"help\": \"checkpoint dir for inference.\"})\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    num_latent: int = field(default=5, metadata={\"help\": \"The number of latent for training or inference.\"})\n",
    "    use_lora: bool = field(default=True, metadata={\"help\": \"Use lora or not.\"})\n",
    "    greedy: bool = field(default=False, metadata={\"help\": \"Greedy decoding during inference.\"})\n",
    "    use_prj: bool = field(default=False, metadata={\"help\": \"Use a prj module after the llm.\"})\n",
    "    prj_dim: int = field(default=2048, metadata={\"help\": \"The hidden dim of the projection module.\"})\n",
    "    prj_dropout: float = field(default=0.0, metadata={\"help\": \"Dropout ratio of the projection module.\"})\n",
    "    prj_no_ln: bool = field(default=False, metadata={\"help\": \"Remove LayerNorm for the projection module.\"})\n",
    "    inf_latent_iterations: int = field(default=1, metadata={\"help\": \"Latent iterations during inference.\"})\n",
    "    inf_num_iterations: int = field(default=5, metadata={\"help\": \"Run multiple times during inference.\"})\n",
    "    remove_eos: bool = field(default=False, metadata={\"help\": \"Do not add <eos> as a delimiter.\"})\n",
    "\n",
    "class CODI(torch.nn.Module):\n",
    "    def __init__(self, model_args, training_args, lora_config):\n",
    "        super().__init__()\n",
    "        self.model_args = model_args\n",
    "        self.training_args = training_args\n",
    "        self.model_name = model_args.model_name_or_path\n",
    "        \n",
    "        # Load the base model\n",
    "        self.codi = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float16,\n",
    "            resume_download=True,\n",
    "        )\n",
    "\n",
    "        ori_vocab_size = self.codi.config.vocab_size\n",
    "        \n",
    "        # Define special tokens\n",
    "        self.pad_token_id = ori_vocab_size\n",
    "        self.bot_id = ori_vocab_size + 1\n",
    "        self.eot_id = ori_vocab_size + 2\n",
    "\n",
    "        self.codi.resize_token_embeddings(ori_vocab_size + 3)\n",
    "        self.dim = self.codi.config.hidden_size\n",
    "\n",
    "        # Apply LoRA configuration\n",
    "        if training_args.use_lora:\n",
    "            self.codi = get_peft_model(self.codi, lora_config)\n",
    "\n",
    "        # Optional Projection Layer\n",
    "        if training_args.use_prj:\n",
    "            self.prj = nn.Sequential(\n",
    "                nn.Dropout(training_args.prj_dropout),\n",
    "                nn.Linear(self.dim, training_args.prj_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(training_args.prj_dim, self.dim),\n",
    "            )\n",
    "            if not training_args.prj_no_ln:\n",
    "                self.prj.add_module(\"ln\", nn.LayerNorm(self.dim))\n",
    "            self.prj.to(self.codi.dtype)\n",
    "\n",
    "    def get_embd(self, model, model_name):\n",
    "        # Helper to get the embedding layer from different model architectures\n",
    "        base_model = model.get_base_model() if hasattr(model, \"get_base_model\") else model\n",
    "        if \"llama\" in model_name.lower() or \"mistral\" in model_name.lower():\n",
    "            return base_model.model.embed_tokens\n",
    "        elif \"phi\" in model_name.lower():\n",
    "             return base_model.model.embed_tokens\n",
    "        elif \"gpt2\" in model_name.lower():\n",
    "            return base_model.transformer.wte\n",
    "        else:\n",
    "            raise NotImplementedError(f\"get_embd not implemented for {model_name}\")\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e1d854-50b0-4f26-88b4-fd02cb9ddf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Helper Functions for Loading and Generation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_and_tokenizer(model_args, training_args):\n",
    "    \"\"\"Loads the model and tokenizer based on the provided arguments.\"\"\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if not model_args.lora_init:\n",
    "        raise ValueError(\"lora_init must be True for this script.\")\n",
    "\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    "    if any(name in model_args.model_name_or_path.lower() for name in [\"llama\", \"mistral\", \"falcon\", \"qwen\", \"phi\"]):\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "        if \"phi\" in model_args.model_name_or_path.lower():\n",
    "             target_modules.extend([\"dense\", \"fc1\", \"fc2\"])\n",
    "    elif any(name in model_args.model_name_or_path.lower() for name in [\"gpt2\"]):\n",
    "        target_modules = [\"c_attn\", \"c_proj\", 'c_fc']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type for LoRA: {model_args.model_name_or_path}.\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=task_type,\n",
    "        inference_mode=False,\n",
    "        r=model_args.lora_r,\n",
    "        lora_alpha=model_args.lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,\n",
    "        init_lora_weights=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing CODI model...\")\n",
    "    model = CODI(model_args, training_args, lora_config)\n",
    "    \n",
    "    try:\n",
    "        # Load fine-tuned adapter weights\n",
    "        ckpt_path = os.path.expanduser(model_args.ckpt_dir)\n",
    "        bin_path = os.path.join(ckpt_path, \"codi.bin\")\n",
    "\n",
    "        if os.path.exists(bin_path):\n",
    "            print(f\"Loading state dict from: {bin_path}\")\n",
    "            state_dict = torch.load(bin_path, map_location=device)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Could not find model weights in {ckpt_path}\")\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Successfully loaded state dict.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading state dictionary: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"left\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.to(torch.bfloat16 if training_args.bf16 else torch.float16)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"CODI Model and tokenizer loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_cache(model, tokenizer, perturbed_question, training_args):\n",
    "    \"\"\"\n",
    "    Runs the model on the perturbed prompt and caches the hidden state activations\n",
    "    for each layer of latent thoughts 1 through 6.\n",
    "\n",
    "    Args:\n",
    "        model: The pre-trained language model.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        perturbed_question (str): The perturbed input prompt.\n",
    "        training_args: Configuration object.\n",
    "\n",
    "     Returns:\n",
    "        tuple(dict, str): A tuple containing:\n",
    "            - activation_cache (dict): A cache of activations, structured as \n",
    "                                       {thought_idx: {layer_idx: tensor, ...}, ...}.\n",
    "            - final_answer (str): The generated final answer from the model.\n",
    "    \"\"\"\n",
    "    # 1. Initialize the Cache\n",
    "    activation_cache = {i: {} for i in range(1, training_args.inf_latent_iterations + 1)}\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Preprocess the Input\n",
    "    inputs = tokenizer(perturbed_question, return_tensors=\"pt\").to(device)\n",
    "    bot_tensor = torch.tensor([model.bot_id], dtype=torch.long, device=device).expand(inputs.input_ids.size(0), 1)\n",
    "    input_ids = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "    attention_mask = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 3. Initial pass for \"Latent Thought 0\"\n",
    "        # This sets up the initial state for the reasoning process.\n",
    "        outputs = model.codi(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "\n",
    "        if training_args.use_prj:\n",
    "            latent_embd = model.prj(latent_embd)\n",
    "\n",
    "        # 4. Main Caching Loop: Iterate through Latent Thoughts 1-6\n",
    "        for thought_idx in range(1, training_args.inf_latent_iterations + 1):\n",
    "            outputs = model.codi(\n",
    "                inputs_embeds=latent_embd,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            \n",
    "            # --- CACHING LOGIC ---\n",
    "            for layer_idx, layer_hidden_state in enumerate(outputs.hidden_states):\n",
    "                # Shape is (1, 1, 2048). Clone to store a clean copy.\n",
    "                activation_cache[thought_idx][layer_idx] = layer_hidden_state.clone().detach()\n",
    "            \n",
    "            # Prepare for the next iteration\n",
    "            past_key_values = outputs.past_key_values\n",
    "            latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "\n",
    "            if training_args.use_prj:\n",
    "                latent_embd = model.prj(latent_embd)\n",
    "        \n",
    "        # --- END OF THOUGHT GENERATION AND CACHING ---\n",
    "        # The 'past_key_values' now contains the state after all thoughts.\n",
    "        # We proceed to generate the final answer.\n",
    "\n",
    "        # 5. Generate the Final Answer from the Latent Thoughts\n",
    "        eot_token_id = torch.tensor([model.eot_id], dtype=torch.long, device=device)\n",
    "        next_input_embeds = model.get_embd(model.codi, model.model_name)(eot_token_id).unsqueeze(0)\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        for _ in range(training_args.model_max_length):\n",
    "            # We no longer need hidden states for the final answer generation\n",
    "            out = model.codi(\n",
    "                inputs_embeds=next_input_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "                output_hidden_states=False \n",
    "            )\n",
    "            \n",
    "            past_key_values = out.past_key_values\n",
    "            current_logits = out.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(current_logits, dim=-1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            generated_token_ids.append(next_token_id.item())\n",
    "            next_input_embeds = model.get_embd(model.codi, model.model_name)(next_token_id).unsqueeze(0)\n",
    "\n",
    "    # 6. Decode the generated tokens into the final string\n",
    "    final_answer = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # 7. Return both the cache and the final answer\n",
    "    return activation_cache, final_answer\n",
    "\n",
    "def analyze_cache(activation_cache):\n",
    "    \"\"\"\n",
    "    Analyzes and prints the structure and tensor shapes of an activation cache.\n",
    "    \"\"\"\n",
    "    if not activation_cache:\n",
    "        print(\"The activation cache is empty.\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Analyzing Activation Cache Structure ---\")\n",
    "    \n",
    "    # Get the list of thought indices (e.g., [1, 2, 3, 4, 5, 6])\n",
    "    thought_indices = sorted(activation_cache.keys())\n",
    "    print(f\"Cached Thought Indices: {thought_indices}\")\n",
    "    print(f\"Total Thoughts Cached: {len(thought_indices)}\\n\")\n",
    "\n",
    "    # Inspect the first thought in detail to show the structure for all\n",
    "    first_thought_idx = thought_indices[0]\n",
    "    first_thought_layers = activation_cache[first_thought_idx]\n",
    "    \n",
    "    layer_indices = sorted(first_thought_layers.keys())\n",
    "    \n",
    "    print(f\"--- Details for Thought Index: {first_thought_idx} ---\")\n",
    "    print(f\"Cached Layer Indices: {layer_indices[0]} to {layer_indices[-1]}\")\n",
    "    print(f\"Total Layers Cached per Thought: {len(layer_indices)}\")\n",
    "    \n",
    "    # Get the shape of the activation tensor from the first layer of the first thought\n",
    "    # This shape will be consistent across all layers and thoughts.\n",
    "    sample_tensor = first_thought_layers[layer_indices[0]]\n",
    "    tensor_shape = sample_tensor.shape\n",
    "    \n",
    "    print(f\"\\nShape of each activation tensor: {tensor_shape}\")\n",
    "    print(f\"  - Batch Size: {tensor_shape[0]}\")\n",
    "    print(f\"  - Sequence Length: {tensor_shape[1]} (Represents a single thought token)\")\n",
    "    print(f\"  - Hidden Dimension: {tensor_shape[2]}\")\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(\"The cache is a dictionary where:\")\n",
    "    print(\"  - Keys are thought indices (integers from 1 to 6).\")\n",
    "    print(\"  - Values are another dictionary for layers.\")\n",
    "    print(\"      - Keys are layer indices (integers from 0 to N).\")\n",
    "    print(f\"      - Values are PyTorch tensors of shape {tensor_shape}.\")\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Helper context manager for cleaner hook management\n",
    "@contextmanager\n",
    "def apply_forward_hook(module, hook_fn):\n",
    "    \"\"\"Context manager to apply and automatically remove a forward hook.\"\"\"\n",
    "    handle = module.register_forward_hook(hook_fn)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "def run_patch(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    training_args,\n",
    "    original_question,\n",
    "    activation_cache,\n",
    "    thought_idx_to_patch,\n",
    "    layer_idx_to_patch\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs inference on the original prompt while patching a specific activation\n",
    "    from the cache at a designated thought and layer.\n",
    "\n",
    "    Args:\n",
    "        model: The pre-trained language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        training_args: Configuration object.\n",
    "        original_question (str): The clean, unperturbed question.\n",
    "        activation_cache (dict): The cache from Phase 1.\n",
    "        thought_idx_to_patch (int): The latent thought step to patch (1-6).\n",
    "        layer_idx_to_patch (int): The layer index to patch (0 to N-1).\n",
    "\n",
    "    Returns:\n",
    "        str: The final generated answer string from the patched run.\n",
    "    \"\"\"\n",
    "    # 1. Retrieve the specific activation to patch from the cache\n",
    "    # This is the \"corrupted\" state we will inject.\n",
    "    cached_activation = activation_cache[thought_idx_to_patch][layer_idx_to_patch]\n",
    "\n",
    "    # --- Hook Definition and Logic ---\n",
    "    # We use a mutable object (a list) as a counter to track the thought index.\n",
    "    # This allows the inner hook function to modify a variable from the outer scope.\n",
    "    thought_counter = [0] \n",
    "\n",
    "    def patch_activation_hook(module, args, output):\n",
    "        # This hook fires every time a forward pass happens on the hooked layer.\n",
    "        # We only want to patch when we are processing the target thought.\n",
    "        \n",
    "        # The first pass is for the prompt + [Begin_Thought], which we count as thought 0.\n",
    "        # The subsequent passes are for thoughts 1, 2, 3, ...\n",
    "        if thought_counter[0] == thought_idx_to_patch:\n",
    "            # The output of a layer is a tuple in some models (e.g., (hidden_state, ...))\n",
    "            # Or just the tensor itself. Let's assume it's the tensor for simplicity.\n",
    "            # We must handle both cases.\n",
    "            if isinstance(output, tuple):\n",
    "                # Modify the first element, which is typically the hidden state.\n",
    "                # Shape is (batch, seq_len, hidden_dim) -> (1, 1, 2048)\n",
    "                output_list = list(output)\n",
    "                output_list[0] = cached_activation\n",
    "                return tuple(output_list)\n",
    "            else:\n",
    "                # If output is just a tensor, replace it directly.\n",
    "                return cached_activation\n",
    "        \n",
    "        # For all other thoughts, do nothing and return the original output.\n",
    "        return output\n",
    "    \n",
    "    # --- Start of Modified Generation Logic ---\n",
    "    model.eval()\n",
    "\n",
    "    # Get the target layer module. The path may need adjustment.\n",
    "    # Example for a standard Hugging Face GPT-2/Llama architecture:\n",
    "    # `model.codi.model.layers[layer_idx_to_patch]`\n",
    "    # We will assume a path like this for the example.\n",
    "    target_layer = model.codi.base_model.model.model.layers[layer_idx_to_patch]\n",
    "\n",
    "    with torch.no_grad(), apply_forward_hook(target_layer, patch_activation_hook):\n",
    "        # 2. Preprocess the ORIGINAL input\n",
    "        inputs = tokenizer(original_question, return_tensors=\"pt\").to(device)\n",
    "        bot_tensor = torch.tensor([model.bot_id], dtype=torch.long, device=device).expand(inputs.input_ids.size(0), 1)\n",
    "        input_ids = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "        attention_mask = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "        # 3. Run \"Latent Thought 0\" Generation\n",
    "        # The hook will fire here, but since thought_counter[0] is 0, it won't patch\n",
    "        # unless thought_idx_to_patch is also 0 (which we have excluded).\n",
    "        outputs = model.codi(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        thought_counter[0] += 1 # Increment after processing thought 0\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "        if training_args.use_prj:\n",
    "            latent_embd = model.prj(latent_embd)\n",
    "\n",
    "        # 4. Iterate through latent thoughts\n",
    "        for i in range(1, training_args.inf_latent_iterations + 1):\n",
    "            # The hook will fire on the forward pass inside this loop.\n",
    "            # If `thought_counter[0]` matches `thought_idx_to_patch`, the swap happens.\n",
    "            outputs = model.codi(\n",
    "                inputs_embeds=latent_embd,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            thought_counter[0] += 1 # Increment after processing each thought\n",
    "            past_key_values = outputs.past_key_values\n",
    "            latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "            if training_args.use_prj:\n",
    "                latent_embd = model.prj(latent_embd)\n",
    "\n",
    "        # 5. Generate the final answer (same as original `generate` function)\n",
    "        eot_token_id = torch.tensor([model.eot_id], dtype=torch.long, device=device)\n",
    "        next_input_embeds = model.get_embd(model.codi, model.model_name)(eot_token_id).unsqueeze(0)\n",
    "        generated_token_ids = []\n",
    "        for _ in range(training_args.model_max_length):\n",
    "            # The hook will continue to fire here but the counter is now too high, so it won't patch.\n",
    "            out = model.codi(\n",
    "                inputs_embeds=next_input_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            past_key_values = out.past_key_values\n",
    "            current_logits = out.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(current_logits, dim=-1)\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            generated_token_ids.append(next_token_id.item())\n",
    "            next_input_embeds = model.get_embd(model.codi, model.model_name)(next_token_id).unsqueeze(0)\n",
    "\n",
    "    # 6. Decode and return the final answer\n",
    "    final_answer = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "    return final_answer\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7ea221-3df5-4981-9fb8-8ef6bc7721c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing CODI model...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named model.safetensors, or pytorch_model.bin, found in directory ./llama.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     12\u001b[39m training_args = TrainingArguments(\n\u001b[32m     13\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./output\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# Dummy output dir\u001b[39;00m\n\u001b[32m     14\u001b[39m     seed=\u001b[32m11\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     use_lora=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m set_seed(training_args.seed)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m codi_model, tokenizer = \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m codi_model \u001b[38;5;129;01mand\u001b[39;00m tokenizer:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll models ready!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mload_model_and_tokenizer\u001b[39m\u001b[34m(model_args, training_args)\u001b[39m\n\u001b[32m     21\u001b[39m lora_config = LoraConfig(\n\u001b[32m     22\u001b[39m     task_type=task_type,\n\u001b[32m     23\u001b[39m     inference_mode=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     init_lora_weights=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing CODI model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m model = \u001b[43mCODI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Load fine-tuned adapter weights\u001b[39;00m\n\u001b[32m     36\u001b[39m     ckpt_path = os.path.expanduser(model_args.ckpt_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mCODI.__init__\u001b[39m\u001b[34m(self, model_args, training_args, lora_config)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_args.model_name_or_path\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Load the base model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28mself\u001b[39m.codi = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m ori_vocab_size = \u001b[38;5;28mself\u001b[39m.codi.config.vocab_size\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Define special tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lambda/nfs/SSSM/latent_reasoning/Scratchpad-Thinking/.venv-cu/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lambda/nfs/SSSM/latent_reasoning/Scratchpad-Thinking/.venv-cu/lib/python3.11/site-packages/transformers/modeling_utils.py:4038\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4033\u001b[39m     logger.warning_once(\n\u001b[32m   4034\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA kernel_config was provided but use_kernels is False; setting use_kernels=True automatically. To suppress this warning, explicitly set use_kernels to True.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4035\u001b[39m     )\n\u001b[32m   4036\u001b[39m     use_kernels = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4038\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4042\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_kwargs_with_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4044\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers_weights\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4047\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4049\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lambda/nfs/SSSM/latent_reasoning/Scratchpad-Thinking/.venv-cu/lib/python3.11/site-packages/transformers/modeling_utils.py:595\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, variant, gguf_file, use_safetensors, user_agent, is_remote_code, transformers_explicit_filename, download_kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    591\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m found in directory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    592\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    593\u001b[39m         )\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    596\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    597\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    598\u001b[39m         )\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n\u001b[32m    600\u001b[39m     archive_file = pretrained_model_name_or_path\n",
      "\u001b[31mOSError\u001b[39m: Error no file named model.safetensors, or pytorch_model.bin, found in directory ./llama."
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration and Interactive Session\n",
    "# --- Configuration is now set directly in the script ---\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"./llama\", # Load base model from local folder\n",
    "    lora_r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_init=True,\n",
    "    ckpt_dir=\"./llama\" # Path to fine-tuned adapter\n",
    ")\n",
    "\n",
    "# Note: TrainingArguments is used for model config, not actual training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\", # Dummy output dir\n",
    "    seed=11,\n",
    "    model_max_length=512,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    greedy=True,\n",
    "    num_latent=6,\n",
    "    use_prj=True,\n",
    "    prj_dim=2048,\n",
    "    prj_no_ln=False,\n",
    "    prj_dropout=0.0,\n",
    "    inf_latent_iterations=6,\n",
    "    remove_eos=True,\n",
    "    use_lora=True,\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "codi_model, tokenizer = load_model_and_tokenizer(model_args, training_args)\n",
    "\n",
    "if codi_model and tokenizer:\n",
    "    print(\"\\nAll models ready!\")\n",
    "else:\n",
    "    print(\"ERROR loading models, read above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fa288d-4c24-49de-824a-7c0023c34f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Interactive Mode ---\n",
      "Enter your question below. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Question:  Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      " Modified Question:  Janet’s ducks lay 11 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating with CODI model...\n",
      "Cache complete\n",
      "\n",
      "--- Modified Answer ---\n",
      "The answer is: 8\n",
      "--------------------\n",
      "\n",
      "--- Patched Answer ---\n",
      "The answer is: 18\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Question:  exit\n"
     ]
    }
   ],
   "source": [
    "#Cell 4: Run Loop\n",
    "if codi_model and tokenizer:\n",
    "    print(\"\\n--- Interactive Mode ---\")\n",
    "    print(\"Enter your question below. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            user_question = input(\"\\n Original Question: \")\n",
    "            if user_question.lower() == 'exit':\n",
    "                break\n",
    "            modif = input(\"\\n Modified Question: \")\n",
    "            print(\"\\nGenerating with CODI model...\")\n",
    "            cache, final_answer = generate_cache(codi_model, tokenizer, modif, training_args)\n",
    "            print(\"Cache complete\")\n",
    "            #original question, cache, thought (1-6), layer (0-15)\n",
    "            patched = run_patch(codi_model, tokenizer, training_args, user_question, cache, 1, 5)\n",
    "            # analyze_cache(cache)exit\n",
    "            \n",
    "            print(\"\\n--- Modified Answer ---\")\n",
    "            print(final_answer.strip())\n",
    "            print(\"--------------------\")\n",
    "\n",
    "            print(\"\\n--- Patched Answer ---\")\n",
    "            print(patched.strip())\n",
    "            print(\"--------------------\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58d95c-a1bb-4c46-8185-640fe09ec40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scratchpad-cu)",
   "language": "python",
   "name": "scratchpad-cu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
